{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "555e50b9-8a11-4001-bf21-416643459022",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Install dependencies\n",
    "# ============================\n",
    "! pip install -U -qq torch torchvision transformers datasets scikit-learn pandas fairlearn evaluate openpyxl ipywidgets jupyterlab_widgets notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3457a0d1-d44a-4c3b-9c99-5ee9be337221",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BERT-based prediction combining text (feedback) and numeric data\n",
    "# Features:\n",
    "# 1) Creates pseudo-labels from 'feedback/text' column using a zero-shot classifier or keyword sentiment heuristics\n",
    "# 2) Builds a PyTorch + Hugging Face BERT classifier that concatenates BERT text embedding with numeric features\n",
    "# 3) Implements a simple adversarial debiasing (gradient reversal + adversary predicting a sensitive attribute)\n",
    "# 4) Monitors fairness metrics using fairlearn and offers a simple reweighting mitigation\n",
    "# 5) Produces CSV output files with predictions and probabilities\n",
    "\n",
    "\n",
    "# --------- 2. Imports and config ----------\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "\n",
    "# For zero-shot / sentiment pseudo-labeling\n",
    "from transformers import pipeline\n",
    "\n",
    "# Fairness\n",
    "from fairlearn.metrics import MetricFrame\n",
    "from fairlearn.metrics import selection_rate, demographic_parity_difference, equalized_odds_difference\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc4369cc-840b-4cc2-a43c-fd78be64acd8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x18d62bea4f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7393c2d6-60ef-4528-8fa8-ec54dc002b3a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Please upload your files (.zip, .csv, .xlsx).\n",
      " If ZIP: It will be extracted automatically.\n",
      " You can select multiple files at once.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb75a174dd3f4b62b001abc0460d6b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.zip,.csv,.xlsx', description='Upload', multiple=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144ec8fe2d3c4bbaa4f9b8a8c8a1d380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6030fbf2a5a415c89023c3646d0002b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47bf79796c054540b87629cef45d32fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Directory for extracted/processed files\n",
    "extract_dir = \"extracted_files\"\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "print(\" Please upload your files (.zip, .csv, .xlsx).\")\n",
    "print(\" If ZIP: It will be extracted automatically.\")\n",
    "print(\" You can select multiple files at once.\")\n",
    "\n",
    "# Widgets\n",
    "uploader = widgets.FileUpload(accept='.zip,.csv,.xlsx', multiple=True)\n",
    "ui_area = widgets.Output()        # For join key dropdowns & merge button\n",
    "results_area = widgets.Output()   # For showing merge results\n",
    "selection_area = widgets.Output() # For selecting text/sensitive columns\n",
    "\n",
    "# Global storage\n",
    "dfs = {}\n",
    "file_paths = {}\n",
    "merged_df = None\n",
    "TEXT_COLS = []\n",
    "SENSITIVE_COLS = []\n",
    "\n",
    "def auto_process(change):\n",
    "    with ui_area:\n",
    "        clear_output()\n",
    "    with results_area:\n",
    "        clear_output()\n",
    "    with selection_area:\n",
    "        clear_output()\n",
    "\n",
    "    global dfs, file_paths\n",
    "    dfs = {}\n",
    "    file_paths = {}\n",
    "\n",
    "    # Clear old extracted files\n",
    "    for root, _, files in os.walk(extract_dir):\n",
    "        for f in files:\n",
    "            os.remove(os.path.join(root, f))\n",
    "\n",
    "    # Save uploaded files\n",
    "    for fileinfo in uploader.value:\n",
    "        filename = fileinfo['name']\n",
    "        content = fileinfo['content']\n",
    "        local_path = os.path.join(extract_dir, filename)\n",
    "        with open(local_path, 'wb') as f:\n",
    "            f.write(content)\n",
    "        print(f\" File {filename} uploaded successfully!\")\n",
    "\n",
    "        # If ZIP, extract and remove original\n",
    "        if filename.lower().endswith('.zip'):\n",
    "            with zipfile.ZipFile(local_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_dir)\n",
    "            os.remove(local_path)\n",
    "\n",
    "    # Read CSV/XLSX files\n",
    "    for root, _, files_in_dir in os.walk(extract_dir):\n",
    "        for f in files_in_dir:\n",
    "            if f.lower().endswith(('.csv', '.xlsx')):\n",
    "                path = os.path.join(root, f)\n",
    "                file_paths[path] = f\n",
    "                if path.lower().endswith('.csv'):\n",
    "                    dfs[path] = pd.read_csv(path)\n",
    "                else:\n",
    "                    dfs[path] = pd.read_excel(path)\n",
    "\n",
    "    if not file_paths:\n",
    "        with results_area:\n",
    "            print(\" No CSV/XLSX files found.\")\n",
    "        return\n",
    "\n",
    "    # Join key selectors\n",
    "    join_key_widgets = {}\n",
    "    for path in file_paths:\n",
    "        cols = dfs[path].columns.tolist()\n",
    "        dropdown = widgets.Dropdown(\n",
    "            options=cols,\n",
    "            description=os.path.basename(path),\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='50%')\n",
    "        )\n",
    "        join_key_widgets[path] = dropdown\n",
    "\n",
    "    join_type_dropdown = widgets.Dropdown(\n",
    "        options=['inner', 'left', 'right', 'outer'],\n",
    "        value='inner',\n",
    "        description='Join Type:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    merge_button = widgets.Button(description=\" Merge Files\", button_style='success')\n",
    "\n",
    "    def merge_action(b):\n",
    "        with results_area:\n",
    "            clear_output()\n",
    "            global merged_df\n",
    "            join_info = [(path, dropdown.value) for path, dropdown in join_key_widgets.items()]\n",
    "            join_type = join_type_dropdown.value\n",
    "\n",
    "            merged_df = None\n",
    "            for i, (path, key) in enumerate(join_info):\n",
    "                df = dfs[path]\n",
    "                if merged_df is None:\n",
    "                    merged_df = df\n",
    "                    main_key = key\n",
    "                else:\n",
    "                    merged_df = pd.merge(merged_df, df, left_on=main_key, right_on=key, how=join_type)\n",
    "\n",
    "            print(\"\\n Merged DataFrame shape:\", merged_df.shape)\n",
    "            display(merged_df.head())\n",
    "\n",
    "        with selection_area:\n",
    "            clear_output()\n",
    "\n",
    "            if merged_df is None or merged_df.empty:\n",
    "                print(\" No merged data to select from.\")\n",
    "                return\n",
    "\n",
    "            col_options = list(merged_df.columns)\n",
    "\n",
    "            text_col_selector = widgets.SelectMultiple(\n",
    "                options=col_options,\n",
    "                description=\"Text Columns\",\n",
    "                style={'description_width': 'initial'},\n",
    "                layout=widgets.Layout(width='50%')\n",
    "            )\n",
    "\n",
    "            sensitive_col_selector = widgets.SelectMultiple(\n",
    "                options=col_options,\n",
    "                description=\"Sensitive Columns\",\n",
    "                style={'description_width': 'initial'},\n",
    "                layout=widgets.Layout(width='50%')\n",
    "            )\n",
    "\n",
    "            confirm_button = widgets.Button(description=\"Confirm Selections\", button_style='info')\n",
    "\n",
    "            def confirm_action(_):\n",
    "                global TEXT_COLS, SENSITIVE_COLS\n",
    "                TEXT_COLS = list(text_col_selector.value)\n",
    "                SENSITIVE_COLS = list(sensitive_col_selector.value)\n",
    "\n",
    "                with selection_area:\n",
    "                    clear_output()\n",
    "                    print(\"Using text columns:\", TEXT_COLS)\n",
    "                    if SENSITIVE_COLS:\n",
    "                        print(\"Using sensitive attribute columns:\", SENSITIVE_COLS)\n",
    "                    else:\n",
    "                        print(\"No sensitive attributes selected. Adversarial debiasing will be skipped.\")\n",
    "\n",
    "            confirm_button.on_click(confirm_action)\n",
    "\n",
    "            display(widgets.VBox([text_col_selector, sensitive_col_selector, confirm_button]))\n",
    "\n",
    "    merge_button.on_click(merge_action)\n",
    "\n",
    "    with ui_area:\n",
    "        display(widgets.VBox(list(join_key_widgets.values()) + [join_type_dropdown, merge_button]))\n",
    "\n",
    "uploader.observe(auto_process, names='value')\n",
    "\n",
    "display(uploader, ui_area, results_area, selection_area)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0e7a711-87c8-4d1f-99de-32920abb32dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Step X: Ask user to select feedback/text columns\\nSENSITIVE_COLS = []\\nprint(\"\\nAvailable columns in merged_df:\")\\nfor idx, col in enumerate(merged_df.columns):\\n    print(f\"{idx}: {col}\")\\n\\ntext_col_indices = input(\"\\nEnter column numbers for feedback/text (comma-separated, e.g., 2 or 2,5): \").strip()\\ntext_col_indices = [int(i.strip()) for i in text_col_indices.split(\",\") if i.strip().isdigit()]\\nTEXT_COLS = [merged_df.columns[i] for i in text_col_indices]\\nprint(\"Using text columns:\", TEXT_COLS)\\n\\n# Step X: Ask user to select sensitive attribute columns for fairness\\nsensitive_col_indices = input(\"\\nEnter column numbers for sensitive attributes (comma-separated, or press Enter to skip): \").strip()\\nif sensitive_col_indices:\\n    sensitive_col_indices = [int(i.strip()) for i in sensitive_col_indices.split(\",\") if i.strip().isdigit()]\\n    SENSITIVE_COLS = [merged_df.columns[i] for i in sensitive_col_indices]\\n    print(\"Using sensitive attribute columns:\", SENSITIVE_COLS)\\nelse:\\n    SENSITIVE_COLS = []\\n    print(\"No sensitive attributes selected. Adversarial debiasing will be skipped.\")\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Step X: Ask user to select feedback/text columns\n",
    "SENSITIVE_COLS = []\n",
    "print(\"\\nAvailable columns in merged_df:\")\n",
    "for idx, col in enumerate(merged_df.columns):\n",
    "    print(f\"{idx}: {col}\")\n",
    "\n",
    "text_col_indices = input(\"\\nEnter column numbers for feedback/text (comma-separated, e.g., 2 or 2,5): \").strip()\n",
    "text_col_indices = [int(i.strip()) for i in text_col_indices.split(\",\") if i.strip().isdigit()]\n",
    "TEXT_COLS = [merged_df.columns[i] for i in text_col_indices]\n",
    "print(\"Using text columns:\", TEXT_COLS)\n",
    "\n",
    "# Step X: Ask user to select sensitive attribute columns for fairness\n",
    "sensitive_col_indices = input(\"\\nEnter column numbers for sensitive attributes (comma-separated, or press Enter to skip): \").strip()\n",
    "if sensitive_col_indices:\n",
    "    sensitive_col_indices = [int(i.strip()) for i in sensitive_col_indices.split(\",\") if i.strip().isdigit()]\n",
    "    SENSITIVE_COLS = [merged_df.columns[i] for i in sensitive_col_indices]\n",
    "    print(\"Using sensitive attribute columns:\", SENSITIVE_COLS)\n",
    "else:\n",
    "    SENSITIVE_COLS = []\n",
    "    print(\"No sensitive attributes selected. Adversarial debiasing will be skipped.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "488ad850-dce8-4c56-a77d-0583e028a324",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available columns in merged_df:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Step 1: Ask user to select target/label column\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAvailable columns in merged_df:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mmerged_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m)))\n\u001b[32m      8\u001b[39m label_choice = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEnter column number for final target column (or press Enter if no column exists): \u001b[39m\u001b[33m\"\u001b[39m).strip()\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m label_choice:\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# Ask user to select target/label column or create one\n",
    "print(\"\\nAvailable columns in merged_df:\")\n",
    "for idx, col in enumerate(merged_df.columns):\n",
    "    print(f\"{idx}: {col}\")\n",
    "\n",
    "label_choice = input(\"\\nEnter column number for final target column (or press Enter if no column exists): \").strip()\n",
    "\n",
    "if label_choice:\n",
    "    LABEL_COL = merged_df.columns[int(label_choice)]\n",
    "    print(\"Using existing column as target column:\", LABEL_COL)\n",
    "else:\n",
    "    LABEL_COL = 'pseudo_target'\n",
    "    print(\"No target column selected. Creating target column as \", LABEL_COL)\n",
    "\n",
    "    # Strategy: try zero-shot classification (\"will stay\" vs \"will leave\")\n",
    "    try:\n",
    "        from transformers import pipeline\n",
    "        print('Initializing zero-shot pipeline (this may download a model) ...')\n",
    "        zsp = pipeline('zero-shot-classification', model='facebook/bart-large-mnli')\n",
    "        candidate_labels = ['will churn/leave the service', 'will stay/continue using the service']\n",
    "\n",
    "        def zs_label(text):\n",
    "            if not isinstance(text, str) or text.strip() == '':\n",
    "                return 0  # default: stay\n",
    "            out = zsp(text, candidate_labels)\n",
    "            top = out['labels'][0]\n",
    "            return 1 if 'leave' in top.lower() or 'churn' in top.lower() else 0\n",
    "\n",
    "        print('Applying zero-shot classifier to feedback (this may take a while).')\n",
    "        merged_df[LABEL_COL] = merged_df[TEXT_COLS[0]].fillna('').astype(str).apply(zs_label)\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Zero-shot pipeline failed (', e, '). Falling back to heuristic keyword rules.')\n",
    "        negative_keywords = ['cancel', 'churn', 'leave', 'switch', 'stop', 'uninstall',\n",
    "                              'refund', 'complain', 'close account', 'sue', 'angry']\n",
    "\n",
    "        def heuristic_label(text):\n",
    "            t = '' if not isinstance(text, str) else text.lower()\n",
    "            if any(k in t for k in negative_keywords):\n",
    "                return 1\n",
    "            return 0\n",
    "\n",
    "        merged_df[LABEL_COL] = merged_df[TEXT_COLS[0]].fillna('').astype(str).apply(heuristic_label)\n",
    "\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(merged_df[LABEL_COL].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a653b99-8061-4a3d-89b8-1a025d01486f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --------- Prepare features: numerical cols + encoding ----------\n",
    "# Build exclude list from text, label, and sensitive columns\n",
    "exclude = set(TEXT_COLS)  # unpack text columns\n",
    "exclude.add(LABEL_COL)    # add label column\n",
    "if SENSITIVE_COLS:\n",
    "    exclude.update(SENSITIVE_COLS)  # unpack sensitive columns\n",
    "\n",
    "# Select numeric columns (excluding ID, text, label, sensitive)\n",
    "numeric_cols = [c for c in merged_df.columns if c not in exclude and pd.api.types.is_numeric_dtype(merged_df[c])]\n",
    "print('Numeric columns used:', numeric_cols)\n",
    "\n",
    "# Categorical columns (non-text, non-label, non-numeric)\n",
    "categorical_cols = [c for c in merged_df.columns\n",
    "                    if c not in exclude\n",
    "                    and c not in numeric_cols\n",
    "                    and merged_df[c].dtype == object]\n",
    "print('Categorical columns:', categorical_cols)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "proc_df = merged_df.copy()\n",
    "\n",
    "# Fill missing numeric\n",
    "for c in numeric_cols:\n",
    "    proc_df[c] = proc_df[c].fillna(proc_df[c].median())\n",
    "\n",
    "# Encode categorical\n",
    "for c in categorical_cols:\n",
    "    proc_df[c] = proc_df[c].fillna('missing').astype(str)\n",
    "    proc_df[c] = proc_df[c].astype('category').cat.codes\n",
    "\n",
    "FEATURE_COLS = numeric_cols + categorical_cols\n",
    "print('Final feature columns to be used as numeric inputs:', FEATURE_COLS)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "if len(FEATURE_COLS) > 0:\n",
    "    proc_df[FEATURE_COLS] = scaler.fit_transform(proc_df[FEATURE_COLS])\n",
    "\n",
    "# Sensitive attribute processing (if present)\n",
    "if SENSITIVE_COLS:\n",
    "    for col in SENSITIVE_COLS:\n",
    "        proc_df[col] = proc_df[col].fillna('missing').astype(str).astype('category').cat.codes\n",
    "\n",
    "# Drop rows with missing text if needed\n",
    "for col in TEXT_COLS:\n",
    "    proc_df[col] = proc_df[col].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbf639a-a5be-4e33-955e-2cfef8c31452",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# --------- Train / Val split ----------\n",
    "train_df, val_df = train_test_split(proc_df, test_size=0.15, random_state=SEED, stratify=proc_df[LABEL_COL] if proc_df[LABEL_COL].nunique()>1 else None)\n",
    "print('Train size:', len(train_df), 'Val size:', len(val_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c6d7fa-d8cf-49a4-b67a-87c413771e01",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --------- Dataset and DataLoader ----------\n",
    "MODEL_NAME = 'bert-base-uncased'  # Change if needed\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "num_epochs=3\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "class ChurnDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        # Combine multiple text columns into one string\n",
    "        if len(TEXT_COLS) > 1:\n",
    "            self.texts = df[TEXT_COLS].astype(str).agg(\" \".join, axis=1).tolist()\n",
    "        else:\n",
    "            self.texts = df[TEXT_COLS[0]].astype(str).tolist()\n",
    "\n",
    "        # Numeric feature array\n",
    "        self.X = df[FEATURE_COLS].values.astype(np.float32) if len(FEATURE_COLS) > 0 else np.zeros((len(df), 0), dtype=np.float32)\n",
    "\n",
    "        # Labels\n",
    "        self.y = df[LABEL_COL].astype(int).values\n",
    "\n",
    "        # Sensitive attributes (combined if multiple)\n",
    "        if SENSITIVE_COLS:\n",
    "            self.sens = df[SENSITIVE_COLS].astype(str).agg(\"-\".join, axis=1).astype(\"category\").cat.codes.values\n",
    "        else:\n",
    "            self.sens = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        txt = self.texts[idx]\n",
    "        enc = TOKENIZER(txt, truncation=True, padding='max_length', max_length=MAX_LEN, return_tensors='pt')\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item['numeric'] = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        item['labels'] = torch.tensor(int(self.y[idx]), dtype=torch.long)\n",
    "        if self.sens is not None:\n",
    "            item['sens'] = torch.tensor(int(self.sens[idx]), dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Example usage:\n",
    "train_ds = ChurnDataset(train_df)\n",
    "val_ds = ChurnDataset(val_df)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997865cf-8bf1-407c-ab09-74374038ce5e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------Model: BERT + numeric MLP + adversary ----------\n",
    "\n",
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.alpha, None\n",
    "\n",
    "\n",
    "class GradientReversalLayer(nn.Module):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.alpha)\n",
    "\n",
    "\n",
    "class BertWithNumericAdversary(nn.Module):\n",
    "    def __init__(self, model_name, numeric_dim, num_sensitive_classes=None):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "\n",
    "        # Classifier head for main task\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size + numeric_dim, max(hidden_size // 2, 32)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(max(hidden_size // 2, 32), 2)\n",
    "        )\n",
    "\n",
    "        # Adversary head for sensitive attribute prediction\n",
    "        self.num_sensitive_classes = num_sensitive_classes\n",
    "        if num_sensitive_classes:\n",
    "            self.grl = GradientReversalLayer(alpha=1.0)\n",
    "            self.adversary = nn.Sequential(\n",
    "                nn.Linear(hidden_size, max(hidden_size // 2, 32)),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(max(hidden_size // 2, 32), num_sensitive_classes)\n",
    "            )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numeric_tensor=None, return_repr=False):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # pooled output from BERT (CLS token)\n",
    "        if hasattr(out, 'pooler_output') and out.pooler_output is not None:\n",
    "            pooled = out.pooler_output\n",
    "        else:\n",
    "            # mean pooling as fallback\n",
    "            last = out.last_hidden_state\n",
    "            pooled = (last * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1, keepdim=True)\n",
    "\n",
    "        # Adversary prediction\n",
    "        adv_logits = None\n",
    "        if self.num_sensitive_classes:\n",
    "            adv_in = self.grl(pooled)\n",
    "            adv_logits = self.adversary(adv_in)\n",
    "\n",
    "        # Concatenate numeric features\n",
    "        if numeric_tensor is None or numeric_tensor.shape[1] == 0:\n",
    "            combined = pooled\n",
    "        else:\n",
    "            combined = torch.cat([pooled, numeric_tensor], dim=1)\n",
    "\n",
    "        logits = self.classifier(combined)\n",
    "\n",
    "        if return_repr:\n",
    "            return logits, adv_logits, pooled\n",
    "        return logits, adv_logits\n",
    "\n",
    "\n",
    "# Create model\n",
    "numeric_dim = len(FEATURE_COLS)\n",
    "\n",
    "if SENSITIVE_COLS:\n",
    "    if isinstance(SENSITIVE_COLS, (list, tuple)):\n",
    "        num_sensitive = proc_df[SENSITIVE_COLS[0]].nunique()\n",
    "    else:\n",
    "        num_sensitive = proc_df[SENSITIVE_COLS].nunique()\n",
    "    num_sensitive = int(num_sensitive)\n",
    "else:\n",
    "    num_sensitive = None\n",
    "\n",
    "model = BertWithNumericAdversary(\n",
    "    MODEL_NAME,\n",
    "    numeric_dim=numeric_dim,\n",
    "    num_sensitive_classes=num_sensitive\n",
    ").to(DEVICE)\n",
    "\n",
    "# Losses and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "adv_criterion = nn.CrossEntropyLoss() if SENSITIVE_COLS else None\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Scheduler\n",
    "\n",
    "\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef1c55b-5311-4efc-a1d5-6808f43ff3e0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# ---------Training loop (with adversarial debiasing) ----------\n",
    "print('Starting training...')\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        numeric = batch['numeric'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits, adv_logits = model(input_ids=input_ids, attention_mask=attention_mask, numeric_tensor=numeric)\n",
    "        loss = criterion(logits, labels)\n",
    "        # adversarial loss: we want adversary to be bad at predicting sensitive attribute -> maximize adversary loss\n",
    "        if SENSITIVE_COLS:\n",
    "            sens = batch['sens'].to(DEVICE)\n",
    "            adv_loss = adv_criterion(adv_logits, sens)\n",
    "            # combine: minimize classification loss + lambda * (-adv_loss) i.e., maximize adv_loss\n",
    "            lambda_adv = 0.5  # tunable\n",
    "            combined_loss = loss - lambda_adv * adv_loss\n",
    "            combined_loss.backward()\n",
    "            total_loss += combined_loss.item()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, avg loss: {avg_loss:.4f}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    ys, preds, probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            numeric = batch['numeric'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            logits, adv_logits = model(input_ids=input_ids, attention_mask=attention_mask, numeric_tensor=numeric)\n",
    "            p = torch.softmax(logits, dim=1)[:,1].detach().cpu().numpy()\n",
    "            predicted = (p >= 0.5).astype(int)\n",
    "            ys.extend(labels.detach().cpu().numpy().tolist())\n",
    "            preds.extend(predicted.tolist())\n",
    "            probs.extend(p.tolist())\n",
    "    print('Val Acc:', accuracy_score(ys, preds), 'F1:', f1_score(ys, preds, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb5b90-61ca-4b6a-ab30-1fc9bb2218aa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --------- Fairness evaluation & simple mitigation ----------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fairlearn.metrics import MetricFrame, selection_rate, demographic_parity_difference, equalized_odds_difference\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Ensure SENSITIVE_COLS has no duplicates\n",
    "if SENSITIVE_COLS:\n",
    "    SENSITIVE_COLS = list(dict.fromkeys(SENSITIVE_COLS))\n",
    "\n",
    "# Remove duplicate columns in DataFrames\n",
    "val_df_copy = val_df.copy()\n",
    "val_df_copy = val_df_copy.loc[:, ~val_df_copy.columns.duplicated()]\n",
    "train_df = train_df.loc[:, ~train_df.columns.duplicated()]\n",
    "\n",
    "# Add predictions to validation DataFrame\n",
    "val_df_copy['pred_prob'] = probs\n",
    "val_df_copy['pred_label'] = preds\n",
    "\n",
    "if SENSITIVE_COLS:\n",
    "    # Handle single vs multi sensitive columns\n",
    "    if len(SENSITIVE_COLS) == 1:\n",
    "        group = val_df_copy[SENSITIVE_COLS[0]]\n",
    "        train_group = train_df[SENSITIVE_COLS[0]]\n",
    "    else:\n",
    "        # Combine multiple sensitive attributes into a single tuple key\n",
    "        group = val_df_copy[SENSITIVE_COLS].astype(str).agg('-'.join, axis=1)\n",
    "        train_group = train_df[SENSITIVE_COLS].astype(str).agg('-'.join, axis=1)\n",
    "\n",
    "    # Compute metrics\n",
    "    metric_frame = MetricFrame(\n",
    "        metrics={\n",
    "            'accuracy': accuracy_score,\n",
    "            'precision': precision_score,\n",
    "            'recall': recall_score,\n",
    "            'selection_rate': selection_rate\n",
    "        },\n",
    "        y_true=val_df_copy[LABEL_COL],\n",
    "        y_pred=val_df_copy['pred_label'],\n",
    "        sensitive_features=group\n",
    "    )\n",
    "\n",
    "    print('Per-group metrics:')\n",
    "    print(metric_frame.by_group)\n",
    "\n",
    "    # Fairness differences\n",
    "    dp_diff = demographic_parity_difference(\n",
    "        y_true=val_df_copy[LABEL_COL],\n",
    "        y_pred=val_df_copy['pred_label'],\n",
    "        sensitive_features=group\n",
    "    )\n",
    "    eqod = equalized_odds_difference(\n",
    "        y_true=val_df_copy[LABEL_COL],\n",
    "        y_pred=val_df_copy['pred_label'],\n",
    "        sensitive_features=group\n",
    "    )\n",
    "    print('Demographic parity difference:', dp_diff)\n",
    "    print('Equalized odds difference:', eqod)\n",
    "\n",
    "    # Simple reweighting mitigation\n",
    "    weights = np.ones(len(train_df))\n",
    "    groups = np.unique(train_group)\n",
    "    target = 1.0 / len(groups)  # uniform target distribution\n",
    "\n",
    "    for gi in groups:\n",
    "        idxs = (train_group == gi).values\n",
    "        cur = idxs.sum() / len(train_group)\n",
    "        if cur > 0:\n",
    "            weights[idxs] = target / cur\n",
    "\n",
    "    print('Example reweighting achieved. You may re-train using sample weights for fairness mitigation.')\n",
    "\n",
    "else:\n",
    "    print('No sensitive attribute available. Fairness monitoring skipped. '\n",
    "          'You can set SENSITIVE_COL manually to a column name and re-run.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f2c8e4-908c-4d10-8f62-a9c2b1c619bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# --------- Produce deliverable prediction CSVs ----------\n",
    "# Use model to predict on the full dataframe and save outputs\n",
    "full_ds = ChurnDataset(proc_df)\n",
    "full_loader = DataLoader(full_ds, batch_size=BATCH_SIZE)\n",
    "all_probs = []\n",
    "all_preds = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        numeric = batch['numeric'].to(DEVICE)\n",
    "        logits, adv_logits = model(input_ids=input_ids, attention_mask=attention_mask, numeric_tensor=numeric)\n",
    "        p = torch.softmax(logits, dim=1)[:,1].detach().cpu().numpy()\n",
    "        predicted = (p >= 0.5).astype(int)\n",
    "        all_probs.extend(p.tolist())\n",
    "        all_preds.extend(predicted.tolist())\n",
    "\n",
    "out_df = df.copy()\n",
    "out_df['pred_churn_prob'] = all_probs\n",
    "#out_df['pred_churn_label'] = all_preds\n",
    "\n",
    "PRED_CSV = '/content/churn_predictions.csv'\n",
    "out_df.to_csv(PRED_CSV, index=False)\n",
    "print('Full Predictions Data Saved To', PRED_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e321159-1a97-4258-a8a5-c4061a1ec0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Define where to save model\n",
    "# ===========================\n",
    "save_dir = input(\"Enter directory path to save model: \").strip()\n",
    "if not save_dir:\n",
    "    save_dir = \"./trained_model\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Update MODEL_NAME to point to saved model\n",
    "# ===========================\n",
    "MODEL_PATH_NAME = f\"{save_dir}/model.pkl\" # Changed extension to .pth\n",
    "TOKENISER_PATH_NAME =  f\"{save_dir}/tokenizer\" # Saved as directory\n",
    "SCALER_PATH_NAME = f\"{save_dir}/numeric_scaler.pkl\"\n",
    "# ===========================\n",
    "# Save model\n",
    "# ===========================\n",
    "if 'model' not in globals():\n",
    "    raise ValueError(\"Trained model object 'model' not found in memory. Please run training first.\")\n",
    "# Use torch.save to save the model's state_dict\n",
    "torch.save(model, MODEL_PATH_NAME)\n",
    "\n",
    "# ===========================\n",
    "# Ensure tokenizer exists (prefer the one used in training)\n",
    "# ===========================\n",
    "if 'TOKENIZER' in globals():\n",
    "    tokenizer = TOKENIZER\n",
    "elif 'tokenizer' in globals():\n",
    "    tokenizer = tokenizer\n",
    "else:\n",
    "    # If no tokenizer object in memory, try loading from MODEL_NAME if defined\n",
    "    if 'MODEL_NAME' in globals():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    else:\n",
    "        raise ValueError(\"No tokenizer found. Ensure you have trained or loaded a model before saving.\")\n",
    "\n",
    "# Save tokenizer from training\n",
    "# Use save_pretrained for Hugging Face tokenizers\n",
    "tokenizer.save_pretrained(TOKENISER_PATH_NAME)\n",
    "\n",
    "# ===========================\n",
    "# Ensure scaler exists\n",
    "# ===========================\n",
    "if 'scaler' not in globals():\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    if 'proc_df' in globals() and 'FEATURE_COLS' in globals() and len(FEATURE_COLS) > 0:\n",
    "        scaler.fit(proc_df[FEATURE_COLS])  # Fit if data is available\n",
    "    else:\n",
    "        print(\"Warning: No numeric features found. Creating an unfitted scaler.\")\n",
    "\n",
    "# Save numeric scaler\n",
    "joblib.dump(scaler, SCALER_PATH_NAME)\n",
    "\n",
    "\n",
    "print(f\" Model, tokenizer, and scaler saved to: {save_dir}\")\n",
    "print(f\" You can now load them with: AutoTokenizer.from_pretrained('{save_dir}')\") # Note: Loading the model requires instantiating the class and loading the state_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
